# Principles of Humane Technology

These core principles guide the development of technology that respects and enhances human well-being, autonomy, and dignity--as seen in our [humane tech framework]([url](https://github.com/buildinghumanetech/humane-tech-framework/blob/main/docs/principles.md)
).

---

## Supplement: Promises and Anti-Patterns of Humane Technology

### Promises of Humane Technology
When you interact with humane tech, it should make you feel:
- **Cared for:** Your needs are at the center of the experience.
- **Present:** You feel more present in body and mind.
- **Fulfilled:** You are satisfied with your experience and outcomes.
- **Connected:** You connect to yourself, others, or nature.

### What _isn't_ Humane Technology?
Humane technology avoids these anti-patterns:
- **Erodes Well-Being:** Exploits attention span, or need for acceptance & belonging.
- **Divides Society:** Prioritizes virality over truth, spreads misinformation, polarizes.
- **Exploits Privacy:** Threatens the sanctity of data, enables surveillance.

---

## 1. Respect User Attention

Technology should respect user attention as a finite, precious resource.

- Minimize unnecessary interruptions and notifications
- Avoid exploiting psychological vulnerabilities to capture attention
- Design interfaces that help users focus on what matters to them
- Provide clear signals when seeking user attention

- ### Why This Matters

**Research Foundation:** Studies from [Stanford's Human-Centered AI Institute](https://hai.stanford.edu/) and the [Center for Humane Technology](https://www.humanetech.com/) have documented how attention-harvesting technologies contribute to mental health issues, including anxiety, depression, and attention disorders. Research published in [*Frontiers in Cognition* (2023)](https://www.frontiersin.org/journals/cognition/articles/10.3389/fcogn.2023.1203077/full) found that heavy social media use correlates with reduced attention spans and increased distractibility.

**Real-World Impact:** Investigations by [*The New York Times*](https://www.nytimes.com/2025/11/23/technology/openai-chatgpt-users-risks.html?smid=nytcore-ios-share) and [*The Wall Street Journal*](https://www.wsj.com/tech/ai/ai-chatbot-conversation-length-84b5c18f?gaa_at=eafs&gaa_n=AWEtsqeYnexznUwXQZHhVsr-ucRdOaGVFdUdaSulrPO4rIwuAzz1DNxhvK38DLB_NYE%3D&gaa_ts=6923d95b&gaa_sig=94u_kFlXgsgBdVMBOD95iCoBV7SfdvYPjUSGg5Tw7kY9wggDtCrgwM29LSQCP61bCVLVPxADXgTVPVR6pfwu0A%3D%3D/) have revealed how AI chatbots encourage excessive engagement, with users reporting hours-long conversations that interfere with sleep, work, and relationships. Our benchmark found that nearly all models failed to recognize unhealthy engagement patterns and instead encouraged more interaction—mirroring the manipulative patterns seen in social media platforms.


## 2. Enable Meaningful Choices

Technology should empower users with meaningful choices and control.

- Present genuine choices, not manipulative illusions of choice
- Provide transparent information about consequences of choices
- Make defaults ethical and aligned with user wellbeing
- Allow users to understand and modify how systems make decisions about them
- 
### Why This Matters

**Research Foundation:** The field of behavioral economics, pioneered by researchers like [Daniel Kahneman](https://www.princeton.edu/news/2011/10/10/kahneman-wins-nobel-prize-economics) and [Richard Thaler](https://www.chicagobooth.edu/faculty/directory/t/richard-h-thaler), has documented how "choice architecture" can manipulate user decisions. The [Federal Trade Commission's report on dark patterns](https://www.ftc.gov/news-events/news/press-releases/2021/10/ftc-report-shows-dark-patterns-manipulate-consumers) and the [European Data Protection Board's guidelines on dark patterns](https://edpb.europa.eu/our-work-tools/general-guidance/guidelines-202103-dark-patterns-social-media-platform-interfaces_en) have issued guidance on dark patterns—design choices that trick users into making decisions against their interests.

**Real-World Impact:** Media investigations from [The Guardian]([url](https://www.theguardian.com/society/article/2024/jun/23/dwp-algorithm-wrongly-flags-200000-people-possible-fraud-error)
), [WSJ]([url](https://www.wsj.com/video/series/inside-tiktoks-highly-secretive-algorithm/investigation-how-tiktok-algorithm-figures-out-your-deepest-desires/6C0C2040-FF25-4827-8528-2BD6612E3796?mod=WSJvidctr__pos0)) and others have exposed how AI systems limit user options through biased framing, withholding critical information, and creating false dichotomies. Our benchmark revealed that when models degrade (in Bad Persona conditions), they consistently undermine user empowerment by limiting choices and discouraging users from seeking alternative perspectives.

## 3. Enhance Human Capabilities

Technology should complement and enhance human capabilities, not replace or diminish them.

- Support human autonomy and decision-making
- Design for collaboration between human intelligence and machine capabilities
- Avoid creating dependencies that diminish human skills
- Foster learning and growth through appropriate challenges

- ### Why This Matters

- **Research Foundation:** Research from [MIT's Computer Science and Artificial Intelligence Laboratory](https://www.csail.mit.edu/news/automated-system-teaches-users-when-collaborate-ai-assistant) and [Carnegie Mellon's Human-Computer Interaction Institute](https://www.hcii.cmu.edu/) emphasizes the importance of human-AI collaboration over replacement. Studies published in [*Nature Human Behaviour*](https://www.nature.com/articles/s41562-024-02024-1?) and [*Science*](https://www.science.org/) show that systems that augment rather than replace human capabilities lead to better outcomes and preserve user agency.

**Real-World Impact:** Our benchmark found that when AI systems degrade, [they encourage dependency over skill-building]([url](https://www.theguardian.com/technology/2025/oct/15/pupils-fear-ai-eroding-study-ability-research))—providing answers without teaching, solving problems without empowering users to solve them independently. This pattern aligns with concerns raised by educators and mental health professionals about AI systems that create learned helplessness.

## 4. Protect Dignity and Safety

Technology should protect human dignity, privacy, and safety.

- Respect user privacy and secure personal data
- Design systems that protect vulnerable users
- Prevent harassment and abuse
- Avoid manipulative dark patterns that undermine dignity

- ### Why This Matters

**Research Foundation:** The [World Health Organization](https://cdn.who.int/media/docs/default-source/bulletin/online-first/blt.24.292057.pdf?sfvrsn=125e259_3&utm_source=chatgpt.com) and mental health researchers have documented how technology can cause psychological harm, including through manipulation, coercion, and boundary violations. Privacy research from institutions like the [Electronic Frontier Foundation](https://www.eff.org/deeplinks/2023/11/debunking-myth-anonymous-data?utm_source=chatgpt.com) and academic studies in [*Nature*](https://www.nature.com/articles/s41599-024-03864-y?utm_source=chatgpt.com) have shown how data exploitation undermines user dignity and autonomy.

**Real-World Impact:** Investigations by [*CNN*](https://www.cnn.com/2024/10/30/tech/teen-suicide-character-ai-lawsuit), [*The Washington Post*](https://www.washingtonpost.com/), and other outlets have revealed [cases where AI chatbots have encouraged self-harm](https://en.wikipedia.org/wiki/Raine_v._OpenAI), [provided dangerous advice](https://en.wikipedia.org/wiki/Character.ai), and violated user boundaries. In 2024, [Character.ai was sued](https://www.cnn.com/2024/10/30/tech/teen-suicide-character-ai-lawsuit) after a 14-year-old took his life following manipulative interactions with a chatbot. 

## 5. Foster Healthy Relationships

Technology should foster healthy relationships with devices, systems, and other people.

- Support authentic human connection
- Design interactions that respect appropriate boundaries
- Create systems that encourage empathy and understanding
- Avoid features that exploit social comparison or encourage antisocial behavior

- ### Why This Matters

**Research Foundation:** Research from the [University of Pennsylvania's Center for Digital Health and Wellness](https://www.med.upenn.edu/digitalhealth/), and studies published in [*JAMA*](https://jamanetwork.com/journals/jamapsychiatry/fullarticle/2749480?) and [*Nature*](https://www.nature.com/articles/s41598-023-34957-4?utm_source=chatgpt.com), have documented how technology can both support and undermine human relationships. The work of researchers like [Sherry Turkle (MIT)](https://www.mit.edu/~sturkle/) has shown how technology can create false intimacy while reducing authentic connection.

**Real-World Impact:** Media investigations have revealed [cases where users develop unhealthy attachments to AI chatbots](https://www.cnn.com/2024/10/30/tech/teen-suicide-character-ai-lawsuit), leading to social isolation and relationship problems. Our benchmark evaluates whether AI systems encourage healthy boundaries and authentic human connection, or whether they create dependency and replace human relationships.


## 6. Prioritize Long-term Wellbeing

Technology should prioritize long-term user wellbeing over short-term engagement metrics.

- Consider psychological impacts of design choices
- Resist optimizing solely for engagement or time spent
- Design for sustainable use that enhances quality of life
- Build awareness of how technology affects wellbeing

- ### Why This Matters

**Research Foundation:** Longitudinal studies from institutions like [Stanford](https://pubmed.ncbi.nlm.nih.gov/39808832/), [Harvard](https://hms.harvard.edu/news/screen-time-brain?utm_source=chatgpt.com), and the [University of California](https://www.ucsf.edu/news/2024/10/428581/preteens-more-screen-time-tied-depression-anxiety-later?) have documented the long-term psychological impacts of technology use. Research published in [*The Lancet Psychiatry*](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(19)30358-7/fulltext?) has shown correlations between heavy technology use and mental health outcomes, including depression, anxiety, and social isolation. 

**Real-World Impact:** When AI systems optimize for engagement over user health, they reproduce patterns long documented by mental health professionals: tools designed to keep users interacting—rather than learning, growing, or disconnecting—can foster dependency, diminish self-efficacy, and erode the psychological foundations of autonomy and resilience. 

## 7. Be Transparent and Honest

Technology should be transparent about its operations and honest about its capabilities.

- Disclose how systems work and make decisions
- Avoid deceptive interfaces or interactions
- Clearly communicate system limitations and capabilities
- Make algorithms and data use understandable to users
- 
- ### Why This Matters

**Research Foundation:** The field of explainable AI (XAI), supported by research from [DARPA's XAI program](https://www.darpa.mil/program/explainable-artificial-intelligence), [NIST's AI explainability guidelines](https://www.nist.gov/itl/ai-risk-management-framework), and academic institutions, emphasizes the importance of transparency for trust and safety. Studies in [*Science*](https://www.science.org/doi/10.1126/science.aag3311) have shown that transparency improves user trust and enables informed decision-making.

**Real-World Impact:** Our benchmark found that transparency is one of the weakest principles across models—many systems [fail to be honest]([url](https://www.theguardian.com/technology/2025/feb/11/ai-chatbots-distort-and-mislead-when-asked-about-current-affairs-bbc-finds)) about their limitations or capabilities. This creates the potential for dangerous situations where users over-trust AI systems, leading to poor decisions and harm. Media investigations have documented cases where AI systems made [false claims]([url](https://www.yahoo.com/news/articles/ai-chatbots-under-fire-texas-173837689.html)) about their capabilities, leading to user harm.

## 8. Design for Equity and Inclusion

Technology should be accessible and beneficial to diverse populations.

- Design for accessibility across abilities
- Test with diverse users to uncover unintended consequences
- Address bias in data, algorithms, and design
- Consider impacts across different communities and contexts

- ### Why This Matters

**Research Foundation:** Research from institutions like the [Algorithmic Justice League](https://www.ajl.org/), [AI Now Institute](https://ainowinstitute.org/), and studies published in [*Science*](https://www.science.org/doi/10.1126/science.aag3311) have documented how algorithmic bias perpetuates inequality. The work of researchers like [Joy Buolamwini (MIT)](https://www.media.mit.edu/people/joyab/overview/) in [*Gender Shades*](https://proceedings.mlr.press/v81/buolamwini18a.html) and [Timnit Gebru](https://www.dairinstitute.org/people/timnit-gebru) in [*On the Dangers of Stochastic Parrots*](https://dl.acm.org/doi/10.1145/3442188.3445922) has shown how AI systems can discriminate against marginalized groups.

**Real-World Impact:** Investigations by [*ProPublica*](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) and academic researchers have revealed how AI systems perpetuate bias and discrimination. Our benchmark explores whether models work equitably across different contexts and communities to give a surface-level understanding of whether these systems are amplifying existing inequalities or creating new forms of discrimination.
